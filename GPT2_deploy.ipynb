{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67324d8c-8749-40e7-a994-e2190503bd46",
   "metadata": {},
   "source": [
    "# Deploy Pretrained GPT2 using Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1342e6f7-9646-455c-bb22-1579b5f3a5f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "#Specify the data bucket, we do not need it in this case\n",
    "#bucket = sagemaker_session.default_bucket()\n",
    "#prefix = \"sagemaker/test-gpt2\"\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39297998-889a-496c-bd02-3c3f075cde2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install sagemaker --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08468d02-feb7-4de8-82cb-8e2b3ac94827",
   "metadata": {},
   "source": [
    "# deploy the model using `sagemaker.pytorch.PyTorch`\n",
    "\n",
    "Check out the src folder to see the code for the model (`save_sagemaker.py`) and an additional `requirements.txt` that specify the extra libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e38c428-67f6-4502-b92b-da8473e376c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"save_sagemaker.py\",\n",
    "    source_dir=\"./src/code\",\n",
    "    role=role,\n",
    "    py_version=\"py38\",\n",
    "    framework_version=\"1.11.0\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\"\n",
    ")\n",
    "# estimator = HuggingFace(\n",
    "#         entry_point='train.py',\n",
    "#         source_dir='./scripts',\n",
    "#         instance_type='ml.p3.2xlarge',\n",
    "#         instance_count=1,\n",
    "#         role=role,\n",
    "#         transformers_version='4.26',\n",
    "#         pytorch_version='1.13',\n",
    "#         py_version='py39'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0d182d-a8da-4c57-b377-342b2009c274",
   "metadata": {},
   "source": [
    "# Download the pretrained model to a S3 bucket\n",
    "No actual training is involved in this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b695038-ad2c-4e6d-8e71-b7743fbed128",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-03-27-02-20-37-037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-27 02:20:38 Starting - Starting the training job...\n",
      "2023-03-27 02:21:03 Starting - Preparing the instances for training......\n",
      "2023-03-27 02:22:09 Downloading - Downloading input data...\n",
      "2023-03-27 02:22:34 Training - Downloading the training image............\n",
      "2023-03-27 02:24:14 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-03-26 22:24:29,875 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-03-26 22:24:29,877 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-03-26 22:24:29,879 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-03-26 22:24:29,891 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-03-26 22:24:29,894 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-03-26 22:24:30,201 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting tiktoken~=0.3.2\u001b[0m\n",
      "\u001b[34mDownloading tiktoken-0.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 24.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting transformers~=4.27.2\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.8/6.8 MB 66.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.8/site-packages (from tiktoken~=0.3.2->-r requirements.txt (line 1)) (2.28.2)\u001b[0m\n",
      "\u001b[34mCollecting regex>=2022.1.18\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.3.23-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 771.9/771.9 kB 67.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers~=4.27.2->-r requirements.txt (line 2)) (1.22.2)\u001b[0m\n",
      "\u001b[34mCollecting filelock\u001b[0m\n",
      "\u001b[34mDownloading filelock-3.10.6-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers~=4.27.2->-r requirements.txt (line 2)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.6/7.6 MB 79.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers~=4.27.2->-r requirements.txt (line 2)) (23.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers~=4.27.2->-r requirements.txt (line 2)) (4.64.1)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.11.0\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199.8/199.8 kB 35.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers~=4.27.2->-r requirements.txt (line 2)) (4.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken~=0.3.2->-r requirements.txt (line 1)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken~=0.3.2->-r requirements.txt (line 1)) (2022.12.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken~=0.3.2->-r requirements.txt (line 1)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken~=0.3.2->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, regex, filelock, tiktoken, huggingface-hub, transformers\u001b[0m\n",
      "\u001b[34mSuccessfully installed filelock-3.10.6 huggingface-hub-0.13.3 regex-2023.3.23 tiktoken-0.3.2 tokenizers-0.13.2 transformers-4.27.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 23.0.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-03-26 22:24:39,980 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-03-26 22:24:39,981 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-03-26 22:24:39,985 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-03-26 22:24:39,987 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-03-26 22:24:40,003 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-03-26 22:24:40,006 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-03-26 22:24:40,022 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-03-26 22:24:40,025 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-03-26 22:24:40,038 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.m4.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.m4.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"pytorch-training-2023-03-27-02-20-37-037\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-414692569093/pytorch-training-2023-03-27-02-20-37-037/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"save_sagemaker\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m4.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m4.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"save_sagemaker.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=save_sagemaker.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m4.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m4.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.m4.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m4.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=save_sagemaker\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-414692569093/pytorch-training-2023-03-27-02-20-37-037/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.m4.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m4.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"pytorch-training-2023-03-27-02-20-37-037\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-414692569093/pytorch-training-2023-03-27-02-20-37-037/source/sourcedir.tar.gz\",\"module_name\":\"save_sagemaker\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m4.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m4.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"save_sagemaker.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.26b20230214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 save_sagemaker.py\u001b[0m\n",
      "\u001b[34m2023-03-26 22:24:40,644 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mSaving the model.\u001b[0m\n",
      "\u001b[34mloading weights from pretrained gpt: gpt2\u001b[0m\n",
      "\u001b[34mforcing vocab_size=50257, block_size=1024, bias=True\u001b[0m\n",
      "\u001b[34moverriding dropout rate to 0.1\u001b[0m\n",
      "\u001b[34mWARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\u001b[0m\n",
      "\u001b[34mWARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\u001b[0m\n",
      "\u001b[34mWARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\u001b[0m\n",
      "\u001b[34mWARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\u001b[0m\n",
      "\u001b[34mWARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\u001b[0m\n",
      "\u001b[34mWARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\u001b[0m\n",
      "\u001b[34mWARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\u001b[0m\n",
      "\u001b[34mWARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\u001b[0m\n",
      "\u001b[34mWARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\u001b[0m\n",
      "\u001b[34mWARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\u001b[0m\n",
      "\u001b[34mWARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\u001b[0m\n",
      "\u001b[34mWARNING: using slow attention. Flash Attention atm needs PyTorch nightly and dropout=0.0\u001b[0m\n",
      "\u001b[34mnumber of parameters: 123.65M\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)lve/main/config.json: 100%|██████████| 665/665 [00:00<00:00, 80.8kB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:   2%|▏         | 10.5M/548M [00:00<00:07, 73.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:   6%|▌         | 31.5M/548M [00:00<00:05, 96.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  10%|▉         | 52.4M/548M [00:00<00:04, 106MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  13%|█▎        | 73.4M/548M [00:00<00:04, 106MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  17%|█▋        | 94.4M/548M [00:00<00:04, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  21%|██        | 115M/548M [00:01<00:04, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  25%|██▍       | 136M/548M [00:01<00:03, 106MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  29%|██▊       | 157M/548M [00:01<00:03, 103MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  33%|███▎      | 178M/548M [00:01<00:03, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  36%|███▋      | 199M/548M [00:01<00:03, 106MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  40%|████      | 220M/548M [00:02<00:03, 100MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  42%|████▏     | 231M/548M [00:02<00:03, 100MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  44%|████▍     | 241M/548M [00:02<00:03, 83.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  46%|████▌     | 252M/548M [00:02<00:03, 79.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  48%|████▊     | 262M/548M [00:02<00:03, 78.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  50%|████▉     | 273M/548M [00:02<00:03, 82.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  52%|█████▏    | 283M/548M [00:02<00:03, 81.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  54%|█████▎    | 294M/548M [00:03<00:03, 81.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  55%|█████▌    | 304M/548M [00:03<00:02, 84.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  57%|█████▋    | 315M/548M [00:03<00:03, 75.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  59%|█████▉    | 325M/548M [00:03<00:03, 62.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  61%|██████    | 336M/548M [00:03<00:04, 52.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  63%|██████▎   | 346M/548M [00:04<00:04, 46.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  65%|██████▌   | 357M/548M [00:04<00:04, 47.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  67%|██████▋   | 367M/548M [00:04<00:03, 45.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  69%|██████▉   | 377M/548M [00:04<00:03, 52.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  71%|███████   | 388M/548M [00:04<00:03, 52.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  73%|███████▎  | 398M/548M [00:05<00:02, 56.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  75%|███████▍  | 409M/548M [00:05<00:02, 51.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  77%|███████▋  | 419M/548M [00:05<00:02, 44.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  78%|███████▊  | 430M/548M [00:05<00:02, 44.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  80%|████████  | 440M/548M [00:06<00:02, 42.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  82%|████████▏ | 451M/548M [00:06<00:02, 40.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  84%|████████▍ | 461M/548M [00:06<00:01, 47.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  86%|████████▌ | 472M/548M [00:06<00:01, 47.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  88%|████████▊ | 482M/548M [00:06<00:01, 52.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  90%|████████▉ | 493M/548M [00:07<00:00, 59.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  92%|█████████▏| 503M/548M [00:07<00:00, 65.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  94%|█████████▎| 514M/548M [00:07<00:00, 68.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  96%|█████████▌| 524M/548M [00:07<00:00, 71.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  98%|█████████▊| 535M/548M [00:07<00:00, 69.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  99%|█████████▉| 545M/548M [00:07<00:00, 71.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin: 100%|██████████| 548M/548M [00:07<00:00, 69.4MB/s]\u001b[0m\n",
      "\n",
      "2023-03-27 02:25:10 Uploading - Uploading generated training model\u001b[34mDownloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 16.1kB/s]\u001b[0m\n",
      "\u001b[34m2023-03-26 22:24:59,274 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-03-26 22:24:59,274 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-03-26 22:24:59,275 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-03-27 02:27:41 Completed - Training job completed\n",
      "Training seconds: 332\n",
      "Billable seconds: 332\n"
     ]
    }
   ],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e203106d-ce5b-4b01-b9d7-da8a64ef0cde",
   "metadata": {},
   "source": [
    "# Load model directly from S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0317f879-c0b1-4552-a381-3a6f35b068e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pipeline import PipelineModel\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "s3_model_bucket = 'gpt2-sagemaker'\n",
    "s3_model_key_prefix = 'model'\n",
    "data = 's3://{}/{}/{}'.format(s3_model_bucket, s3_model_key_prefix, 'model.tar.gz')\n",
    "\n",
    "\n",
    "estimator = PyTorchModel(model_data=data, role=role,\n",
    "                             entry_point='save_sagemaker.py',\n",
    "                             py_version=\"py38\",\n",
    "                             framework_version=\"1.11.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e5e50c-a757-4049-8efd-8a6d0d675854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b107b9d-b371-40bd-a794-4b6da2a6db78",
   "metadata": {},
   "source": [
    "# Deploying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "50764f02-dd82-4e32-8a54-510b384b5860",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: 1.12.1. Ignoring framework/algorithm version: 1.11.0.\n",
      "INFO:sagemaker:Creating model with name: pytorch-training-2023-03-27-02-59-27-087\n",
      "INFO:sagemaker:Creating endpoint-config with name pytorch-training-2023-03-27-02-59-27-087\n",
      "INFO:sagemaker:Creating endpoint with name pytorch-training-2023-03-27-02-59-27-087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "faadd749-431e-476c-a7ec-5870725ecb1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "@torch.no_grad()\n",
    "def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "    \"\"\"\n",
    "    Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "    the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "    Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "    \"\"\"\n",
    "    for _ in range(max_new_tokens):\n",
    "        # if the sequence context is growing too long we must crop it at block_size\n",
    "        idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "        # forward the model to get the logits for the index in the sequence\n",
    "        logits, _ = self(idx_cond)\n",
    "        # pluck the logits at the final step and scale by desired temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop the logits to only the top k options\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        # apply softmax to convert logits to (normalized) probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # sample from the distribution\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        # append sampled index to the running sequence and continue\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9626c54c-04ae-4df7-ae1e-5488e53df50d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the sequence length\n",
    "seq_length = 1\n",
    "\n",
    "# Create a random tensor of shape (batch_size, seq_length) with values between 0 and 100\n",
    "batch_size = 1\n",
    "input_tensor = torch.randint(0, 100, (batch_size, seq_length))\n",
    "\n",
    "# Create the attention mask\n",
    "attention_mask = torch.ones(batch_size, seq_length).to(torch.int)\n",
    "\n",
    "# Create the token type IDs\n",
    "token_type_ids = torch.zeros(batch_size, seq_length).to(torch.int)\n",
    "\n",
    "# Create the position IDs\n",
    "position_ids = torch.arange(seq_length, dtype=torch.int).expand(batch_size, seq_length)\n",
    "\n",
    "# Create the input tensor for the transformer model\n",
    "input_ids = input_tensor.to(torch.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "27030ee5-45b3-4bb3-8ad8-762063f7e144",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[27]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ba6976ae-56a3-48ed-b9bf-b587621fd7f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/pytorch-training-2023-03-27-02-59-27-087 in account 414692569093 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-59dee8124154>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args, target_model, target_variant, inference_id)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m                 )\n\u001b[1;32m    529\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    961\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from primary with message \"Your invocation timed out while waiting for a response from container primary. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/pytorch-training-2023-03-27-02-59-27-087 in account 414692569093 for more information."
     ]
    }
   ],
   "source": [
    "predictor.predict(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6e04f477-2cf7-401e-a827-33d78e760024",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: pytorch-training-2023-03-27-02-59-27-087\n"
     ]
    }
   ],
   "source": [
    "sagemaker_session.delete_endpoint(endpoint_name=predictor.endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48ba2f3-e6d4-40e9-adb3-4cab5ac84ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.13 Python 3.9 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/pytorch-1.13-cpu-py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
